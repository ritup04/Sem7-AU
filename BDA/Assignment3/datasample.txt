Hadoop, is an open-source framework that allows for the distributed processing of large data, sets
across clusters of computers using simple programming, models,
It is designed to scale up from single servers to thousands of machines each offering local computation and storage
Rather than rely on hardware to deliver high availability the framework itself is designed to detect and handle failures at the application layer
The MapReduce programming model is used to process data in parallel
It consists of a Mapper phase and a Reducer phase
The Mapper breaks input data into key-value pairs while the Reducer aggregates these results
MapReduce is at the core of many big data processing tasks in the industry
Hadoop along with other tools like Spark Hive and Pig has revolutionized how we store and analyze massive datasets,
