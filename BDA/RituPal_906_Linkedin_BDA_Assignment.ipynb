{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Name: Ritu Pal\n",
        "#### USN: 1AUA23BCS906"
      ],
      "metadata": {
        "id": "XsDG7c013Jt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "import random\n",
        "from faker import Faker\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Employee Job Recommendation System\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "fake = Faker()"
      ],
      "metadata": {
        "id": "459m5UUcqemg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Employee Data Simulation & RDD Operations\n",
        "\n",
        "## You are asked to simulate a dataset of 1000 employees with the following attributes:\n",
        "\n",
        "* employee_id (unique ID)\n",
        "* user_name\n",
        "* city\n",
        "* age\n",
        "* ctc (in LPA)\n",
        "* experience_years\n",
        "* skillset\n",
        "* company_name\n",
        "* linkedin_profile"
      ],
      "metadata": {
        "id": "xQAG3ZoGt1_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Generate dataset and create RDD in PySpark"
      ],
      "metadata": {
        "id": "n6ABFirbt0T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Q1: EMPLOYEE DATA SIMULATION & RDD OPERATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define data pools\n",
        "cities = [\"Ahmedabad\", \"Mumbai\", \"Bangalore\", \"Delhi\", \"Pune\", \"Hyderabad\", \"Chennai\", \"Kolkata\"]\n",
        "skillsets_pool = [\n",
        "    \"Machine Learning\", \"Data Science\", \"Python\", \"Java\", \"React\",\n",
        "    \"Node.js\", \"AWS\", \"Docker\", \"Kubernetes\", \"SQL\", \"MongoDB\",\n",
        "    \"Deep Learning\", \"NLP\", \"Computer Vision\", \"DevOps\", \"Angular\"\n",
        "]\n",
        "companies = [\"TCS\", \"Infosys\", \"Wipro\", \"Google\", \"Amazon\", \"Microsoft\",\n",
        "              \"Accenture\", \"Cognizant\", \"HCL\", \"Tech Mahindra\"]\n",
        "\n",
        "def generate_employee(emp_id):\n",
        "    \"\"\"Generate a single employee record\"\"\"\n",
        "    name = fake.name()\n",
        "    city = random.choice(cities)\n",
        "    age = random.randint(22, 55)\n",
        "    ctc = round(random.uniform(3.0, 30.0), 2)\n",
        "    exp = random.randint(0, 20)\n",
        "    # Each employee has 2-5 skills\n",
        "    num_skills = random.randint(2, 5)\n",
        "    skills = random.sample(skillsets_pool, num_skills)\n",
        "    company = random.choice(companies)\n",
        "    linkedin = f\"https://linkedin.com/in/{name.lower().replace(' ', '-')}-{emp_id}\"\n",
        "\n",
        "    return {\n",
        "        'employee_id': emp_id,\n",
        "        'user_name': name,\n",
        "        'city': city,\n",
        "        'age': age,\n",
        "        'ctc': ctc,\n",
        "        'experience_years': exp,\n",
        "        'skillset': skills,\n",
        "        'company_name': company,\n",
        "        'linkedin_profile': linkedin\n",
        "    }\n",
        "\n",
        "# Generate 1000 employees\n",
        "employee_data = [generate_employee(i) for i in range(1, 1001)]\n",
        "\n",
        "# Create RDD\n",
        "employee_rdd = sc.parallelize(employee_data)\n",
        "\n",
        "print(f\"\\nTotal Employees Generated: {employee_rdd.count()}\")\n",
        "print(\"\\nSample Employee Records:\")\n",
        "for emp in employee_rdd.take(3):\n",
        "    print(f\"  ID: {emp['employee_id']}, Name: {emp['user_name']}, \"\n",
        "          f\"City: {emp['city']}, CTC: {emp['ctc']} LPA, \"\n",
        "          f\"Exp: {emp['experience_years']} yrs, Skills: {emp['skillset'][:2]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roJiHPOjtrh2",
        "outputId": "a158f6ee-384e-4e02-b6b4-967f8cb14646"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Q1: EMPLOYEE DATA SIMULATION & RDD OPERATIONS\n",
            "================================================================================\n",
            "\n",
            "Total Employees Generated: 1000\n",
            "\n",
            "Sample Employee Records:\n",
            "  ID: 1, Name: Jose Ramirez, City: Pune, CTC: 14.75 LPA, Exp: 6 yrs, Skills: ['Python', 'Node.js']\n",
            "  ID: 2, Name: Andrea Edwards, City: Pune, CTC: 20.44 LPA, Exp: 5 yrs, Skills: ['AWS', 'Deep Learning']\n",
            "  ID: 3, Name: Greg Hines, City: Pune, CTC: 9.12 LPA, Exp: 1 yrs, Skills: ['Angular', 'Deep Learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Divide RDD into 3 clusters (e.g., based on city, skillset, or experience)"
      ],
      "metadata": {
        "id": "LgeIEMxht72G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Task 2: Dividing Employees into 3 Clusters\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Cluster by experience level\n",
        "def assign_cluster(emp):\n",
        "    exp = emp['experience_years']\n",
        "    if exp <= 3:\n",
        "        return ('Junior', emp)\n",
        "    elif exp <= 10:\n",
        "        return ('Mid-Level', emp)\n",
        "    else:\n",
        "        return ('Senior', emp)\n",
        "\n",
        "clustered_rdd = employee_rdd.map(assign_cluster)\n",
        "\n",
        "# Count employees in each cluster\n",
        "cluster_counts = clustered_rdd.countByKey()\n",
        "print(\"\\nCluster Distribution:\")\n",
        "for cluster, count in sorted(cluster_counts.items()):\n",
        "    print(f\"  {cluster}: {count} employees\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVCjVlTmt9B1",
        "outputId": "54970602-022b-4637-9bfe-690cf11665a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Task 2: Dividing Employees into 3 Clusters\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cluster Distribution:\n",
            "  Junior: 181 employees\n",
            "  Mid-Level: 340 employees\n",
            "  Senior: 479 employees\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Apply filter transformation\n",
        "* Experience > 3 years\n",
        "* CTC < 15 LPA\n",
        "* City = Ahmedabad\n",
        "* Skillset contains Machine Learning"
      ],
      "metadata": {
        "id": "rpvIhc4xuBkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Task 3: Filtered Employees (Exp > 3, CTC < 15, City=Ahmedabad, ML skill)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "filtered_employees = employee_rdd.filter(\n",
        "    lambda emp: emp['experience_years'] > 3 and\n",
        "                emp['ctc'] < 15 and\n",
        "                emp['city'] == 'Ahmedabad' and\n",
        "                'Machine Learning' in emp['skillset']\n",
        ")\n",
        "\n",
        "filtered_count = filtered_employees.count()\n",
        "print(f\"\\nFiltered Employees Count: {filtered_count}\")\n",
        "if filtered_count > 0:\n",
        "    print(\"\\nSample Filtered Employees:\")\n",
        "    for emp in filtered_employees.take(5):\n",
        "        print(f\"  {emp['user_name']} - CTC: {emp['ctc']} LPA, \"\n",
        "              f\"Exp: {emp['experience_years']} yrs, Skills: {emp['skillset']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV-bZkuPuC06",
        "outputId": "e53868de-6b34-4137-e2f7-743a1fe21f3f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Task 3: Filtered Employees (Exp > 3, CTC < 15, City=Ahmedabad, ML skill)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Filtered Employees Count: 12\n",
            "\n",
            "Sample Filtered Employees:\n",
            "  Sharon Baker - CTC: 6.57 LPA, Exp: 16 yrs, Skills: ['Kubernetes', 'Deep Learning', 'Machine Learning']\n",
            "  Amy Gonzalez - CTC: 10.54 LPA, Exp: 4 yrs, Skills: ['React', 'Machine Learning', 'MongoDB', 'Computer Vision', 'Angular']\n",
            "  Kimberly Richardson - CTC: 11.91 LPA, Exp: 8 yrs, Skills: ['Machine Learning', 'React', 'Docker', 'DevOps']\n",
            "  Amy Harris - CTC: 4.06 LPA, Exp: 10 yrs, Skills: ['Node.js', 'Machine Learning', 'Python', 'Java']\n",
            "  Melissa Stanley - CTC: 7.26 LPA, Exp: 20 yrs, Skills: ['Java', 'Computer Vision', 'Machine Learning', 'AWS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2: Job Posting Data & RDD Analysis\n",
        "## Simulate a dataset of 5000 job postings with the following attributes:\n",
        "\n",
        "* job_id\n",
        "* job_role\n",
        "* job_skillset\n",
        "* experience_required (in years)\n",
        "* ctc_expected (in LPA)\n",
        "* city\n",
        "* company_name"
      ],
      "metadata": {
        "id": "_H5h0eGsuRfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Create RDD from this dataset"
      ],
      "metadata": {
        "id": "6LKUxdyixjpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Q2: JOB POSTING DATA & RDD ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "job_roles = [\n",
        "    \"Data Scientist\", \"ML Engineer\", \"Software Engineer\", \"DevOps Engineer\",\n",
        "    \"Full Stack Developer\", \"Backend Developer\", \"Frontend Developer\",\n",
        "    \"Data Engineer\", \"AI Researcher\", \"Cloud Architect\", \"Product Manager\"\n",
        "]\n",
        "\n",
        "def generate_job(job_id):\n",
        "    \"\"\"Generate a single job posting\"\"\"\n",
        "    role = random.choice(job_roles)\n",
        "    num_skills = random.randint(2, 4)\n",
        "    skills = random.sample(skillsets_pool, num_skills)\n",
        "    exp_required = random.randint(0, 15)\n",
        "    ctc_expected = round(random.uniform(5.0, 35.0), 2)\n",
        "    city = random.choice(cities)\n",
        "    company = random.choice(companies)\n",
        "\n",
        "    return {\n",
        "        'job_id': job_id,\n",
        "        'job_role': role,\n",
        "        'job_skillset': skills,\n",
        "        'experience_required': exp_required,\n",
        "        'ctc_expected': ctc_expected,\n",
        "        'city': city,\n",
        "        'company_name': company\n",
        "    }\n",
        "\n",
        "# Generate 5000 jobs\n",
        "job_data = [generate_job(i) for i in range(1, 5001)]\n",
        "\n",
        "# Create RDD\n",
        "job_rdd = sc.parallelize(job_data)\n",
        "\n",
        "print(f\"\\nTotal Job Postings Generated: {job_rdd.count()}\")\n",
        "print(\"\\nSample Job Postings:\")\n",
        "for job in job_rdd.take(3):\n",
        "    print(f\"  ID: {job['job_id']}, Role: {job['job_role']}, \"\n",
        "          f\"City: {job['city']}, CTC: {job['ctc_expected']} LPA, \"\n",
        "          f\"Exp Required: {job['experience_required']} yrs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ki2nnpnuUqu",
        "outputId": "85c5de7f-25be-41bb-92f0-1e84dfcac4c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Q2: JOB POSTING DATA & RDD ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Total Job Postings Generated: 5000\n",
            "\n",
            "Sample Job Postings:\n",
            "  ID: 1, Role: ML Engineer, City: Mumbai, CTC: 29.34 LPA, Exp Required: 7 yrs\n",
            "  ID: 2, Role: Cloud Architect, City: Kolkata, CTC: 13.75 LPA, Exp Required: 3 yrs\n",
            "  ID: 3, Role: ML Engineer, City: Ahmedabad, CTC: 28.35 LPA, Exp Required: 9 yrs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2a: Find all jobs in Ahmedabad"
      ],
      "metadata": {
        "id": "3j8GDlpHuacN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Task 2a: Jobs in Ahmedabad\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "ahmedabad_jobs = job_rdd.filter(lambda job: job['city'] == 'Ahmedabad')\n",
        "ahmedabad_count = ahmedabad_jobs.count()\n",
        "print(f\"\\nJobs in Ahmedabad: {ahmedabad_count}\")\n",
        "print(\"\\nSample Jobs:\")\n",
        "for job in ahmedabad_jobs.take(3):\n",
        "    print(f\"  {job['job_role']} at {job['company_name']} - \"\n",
        "          f\"CTC: {job['ctc_expected']} LPA, Exp: {job['experience_required']} yrs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OMy28-uueGX",
        "outputId": "21063478-b649-428b-8f74-f18cccef48ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Task 2a: Jobs in Ahmedabad\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Jobs in Ahmedabad: 616\n",
            "\n",
            "Sample Jobs:\n",
            "  ML Engineer at Google - CTC: 28.35 LPA, Exp: 9 yrs\n",
            "  Frontend Developer at Wipro - CTC: 26.59 LPA, Exp: 10 yrs\n",
            "  ML Engineer at Accenture - CTC: 11.33 LPA, Exp: 9 yrs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2b: Filter jobs (exp > 2, CTC < 20)"
      ],
      "metadata": {
        "id": "Vq0Cpw-FuhAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Task 2b: Jobs with Exp > 2 years and CTC < 20 LPA\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "filtered_jobs = job_rdd.filter(\n",
        "    lambda job: job['experience_required'] > 2 and job['ctc_expected'] < 20\n",
        ")\n",
        "filtered_jobs_count = filtered_jobs.count()\n",
        "print(f\"\\nFiltered Jobs Count: {filtered_jobs_count}\")\n",
        "print(\"\\nSample Jobs:\")\n",
        "for job in filtered_jobs.take(3):\n",
        "    print(f\"  {job['job_role']} - CTC: {job['ctc_expected']} LPA, \"\n",
        "          f\"Exp Required: {job['experience_required']} yrs, City: {job['city']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3EgAtzmvaum",
        "outputId": "89e28810-798f-42a3-ff7c-3075b71f11df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Task 2b: Jobs with Exp > 2 years and CTC < 20 LPA\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Filtered Jobs Count: 2096\n",
            "\n",
            "Sample Jobs:\n",
            "  Cloud Architect - CTC: 13.75 LPA, Exp Required: 3 yrs, City: Kolkata\n",
            "  Software Engineer - CTC: 16.41 LPA, Exp Required: 6 yrs, City: Delhi\n",
            "  AI Researcher - CTC: 12.42 LPA, Exp Required: 11 yrs, City: Mumbai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2c: Count jobs by city"
      ],
      "metadata": {
        "id": "FlAOETgUvd7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Task 2c: Job Postings Count by City\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "jobs_by_city = job_rdd.map(lambda job: (job['city'], 1)).reduceByKey(lambda a, b: a + b)\n",
        "print(\"\\nJobs per City:\")\n",
        "for city, count in sorted(jobs_by_city.collect(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"  {city}: {count} jobs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSqiEJYGvhte",
        "outputId": "c9de085f-4aca-4f7e-8013-29eb72adae4c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Task 2c: Job Postings Count by City\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Jobs per City:\n",
            "  Chennai: 659 jobs\n",
            "  Kolkata: 637 jobs\n",
            "  Pune: 631 jobs\n",
            "  Delhi: 630 jobs\n",
            "  Mumbai: 629 jobs\n",
            "  Ahmedabad: 616 jobs\n",
            "  Hyderabad: 602 jobs\n",
            "  Bangalore: 596 jobs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3: Job Recommendation System Using RDD Joins\n",
        "## Using the employee dataset (Q1) and job dataset (Q2), design a recommendation mechanism."
      ],
      "metadata": {
        "id": "cG39Q7davkiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_skillset_match(emp_skills, job_skills):\n",
        "    \"\"\"Calculate percentage of job skills matched by employee\"\"\"\n",
        "    if not job_skills:\n",
        "        return 0.0\n",
        "    matched = len(set(emp_skills) & set(job_skills))\n",
        "    return matched / len(job_skills)\n",
        "\n",
        "def calculate_recommendation_score(emp, job):\n",
        "    \"\"\"Calculate weighted recommendation score\"\"\"\n",
        "    # Weight priorities as specified in Task 3\n",
        "    SKILLSET_WEIGHT = 0.4    # Highest weight\n",
        "    EXPERIENCE_WEIGHT = 0.3  # Second highest weight\n",
        "    CTC_WEIGHT = 0.2         # Third highest weight\n",
        "    CITY_WEIGHT = 0.1        # Fourth highest weight\n",
        "\n",
        "    # Skillset match score (0 to 1)\n",
        "    skillset_score = calculate_skillset_match(emp['skillset'], job['job_skillset'])\n",
        "\n",
        "    # Experience fit score (1 if qualified, 0.5 if overqualified, 0 if underqualified)\n",
        "    if emp['experience_years'] >= job['experience_required']:\n",
        "        exp_diff = emp['experience_years'] - job['experience_required']\n",
        "        # Penalize being too overqualified\n",
        "        experience_score = max(0.5, 1.0 - (exp_diff * 0.05))\n",
        "    else:\n",
        "        experience_score = 0.0\n",
        "\n",
        "    # CTC fit score (1 if employee CTC <= job CTC, scaled down if not)\n",
        "    if emp['ctc'] <= job['ctc_expected']:\n",
        "        ctc_score = 1.0\n",
        "    else:\n",
        "        # Penalize if employee expects more than job offers\n",
        "        ctc_diff = emp['ctc'] - job['ctc_expected']\n",
        "        ctc_score = max(0.0, 1.0 - (ctc_diff / 10.0))\n",
        "\n",
        "    # City match score (1 if same city, 0 otherwise)\n",
        "    city_score = 1.0 if emp['city'] == job['city'] else 0.0\n",
        "\n",
        "    # Calculate weighted score\n",
        "    total_score = (\n",
        "        skillset_score * SKILLSET_WEIGHT +\n",
        "        experience_score * EXPERIENCE_WEIGHT +\n",
        "        ctc_score * CTC_WEIGHT +\n",
        "        city_score * CITY_WEIGHT\n",
        "    )\n",
        "\n",
        "    return total_score"
      ],
      "metadata": {
        "id": "pRoz4ryE0W3X"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Map employee skillsets with job skill requirements"
      ],
      "metadata": {
        "id": "-IJrFYpD0XvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTask 1: Mapping employee skillsets with job skill requirements...\")\n",
        "\n",
        "def generate_recommendations(emp):\n",
        "    \"\"\"Generate recommendations for a single employee\"\"\"\n",
        "    emp_id = emp['employee_id']\n",
        "    recommendations = []\n",
        "\n",
        "    # Task 1 & 2: Map skillsets and filter jobs where skillset matches\n",
        "    for job in job_data:\n",
        "        # Task 2: Only recommend if skillset matches (at least one skill in common)\n",
        "        if set(emp['skillset']) & set(job['job_skillset']):\n",
        "            # Task 3: Calculate ranking weight based on multiple factors\n",
        "            score = calculate_recommendation_score(emp, job)\n",
        "            if score > 0:\n",
        "                recommendations.append((score, job))\n",
        "\n",
        "    # Task 4: Sort by weight and take top 3 recommendations\n",
        "    recommendations.sort(key=lambda x: x[0], reverse=True)\n",
        "    top_3 = recommendations[:3]\n",
        "\n",
        "    return (emp_id, emp, top_3)\n",
        "\n",
        "# Generate recommendations for all employees using RDD map operation\n",
        "recommendations_rdd = employee_rdd.map(generate_recommendations)\n",
        "\n",
        "print(\"✓ Task 1 Complete: Employee skillsets mapped with job requirements\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9tsIROt0bWr",
        "outputId": "c1f20a80-ed3f-41b9-9341-9564e9e5b7d1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Task 1: Mapping employee skillsets with job skill requirements...\n",
            "✓ Task 1 Complete: Employee skillsets mapped with job requirements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Recommend jobs only if skillset matches"
      ],
      "metadata": {
        "id": "6pUtgZoI0gL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTask 2: Jobs recommended only when skillset matches\")\n",
        "print(\"         (Filtering applied - no recommendations without skill overlap)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPCWGRPa0i93",
        "outputId": "3bae4bc1-7c33-482e-dd1b-debb12d607e7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Task 2: Jobs recommended only when skillset matches\n",
            "         (Filtering applied - no recommendations without skill overlap)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Ranking weights assigned based on multiple factors"
      ],
      "metadata": {
        "id": "teagpKUg0nyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTask 3: Ranking weights assigned:\")\n",
        "print(\"         • Skillset match: 40% (highest weight)\")\n",
        "print(\"         • Experience fit: 30% (second highest weight)\")\n",
        "print(\"         • CTC expectation: 20% (third highest weight)\")\n",
        "print(\"         • City match: 10% (fourth highest weight)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P-_wloF0r-x",
        "outputId": "1d3fdbe0-d497-4210-91dc-0df08a85ccec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Task 3: Ranking weights assigned:\n",
            "         • Skillset match: 40% (highest weight)\n",
            "         • Experience fit: 30% (second highest weight)\n",
            "         • CTC expectation: 20% (third highest weight)\n",
            "         • City match: 10% (fourth highest weight)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4: Output top 3 recommended jobs for each employee"
      ],
      "metadata": {
        "id": "ghGJwqh90xms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Task 4: Top 3 Recommended Jobs for Sample Employees (Ranked by Weight)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "sample_recommendations = recommendations_rdd.take(5)\n",
        "\n",
        "for emp_id, emp, recs in sample_recommendations:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Employee: {emp['user_name']} (ID: {emp_id})\")\n",
        "    print(f\"Current: {emp['city']}, CTC: {emp['ctc']} LPA, \"\n",
        "          f\"Exp: {emp['experience_years']} yrs\")\n",
        "    print(f\"Skills: {', '.join(emp['skillset'][:3])}\")\n",
        "    print(f\"\\nTop 3 Recommended Jobs:\")\n",
        "\n",
        "    for rank, (score, job) in enumerate(recs, 1):\n",
        "        skill_match = calculate_skillset_match(emp['skillset'], job['job_skillset'])\n",
        "        print(f\"\\n  Rank {rank} - Score: {score:.3f}\")\n",
        "        print(f\"    Job: {job['job_role']} at {job['company_name']}\")\n",
        "        print(f\"    Location: {job['city']}, CTC: {job['ctc_expected']} LPA\")\n",
        "        print(f\"    Exp Required: {job['experience_required']} yrs\")\n",
        "        print(f\"    Required Skills: {', '.join(job['job_skillset'])}\")\n",
        "        print(f\"    Skill Match: {skill_match*100:.1f}%\")\n",
        "\n",
        "# Statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RECOMMENDATION STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "total_recommendations = recommendations_rdd.map(\n",
        "    lambda x: len(x[2])\n",
        ").reduce(lambda a, b: a + b)\n",
        "\n",
        "employees_with_recs = recommendations_rdd.filter(\n",
        "    lambda x: len(x[2]) > 0\n",
        ").count()\n",
        "\n",
        "avg_recs = total_recommendations / employee_rdd.count()\n",
        "\n",
        "print(f\"\\nTotal Employees: {employee_rdd.count()}\")\n",
        "print(f\"Employees with Recommendations: {employees_with_recs}\")\n",
        "print(f\"Average Recommendations per Employee: {avg_recs:.2f}\")\n",
        "\n",
        "# Find employees with highest scoring recommendations\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Employees with Best Job Matches\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "best_matches = recommendations_rdd.filter(\n",
        "    lambda x: len(x[2]) > 0\n",
        ").map(\n",
        "    lambda x: (x[0], x[1], x[2][0][0])  # emp_id, emp, best_score\n",
        ").sortBy(\n",
        "    lambda x: x[2], ascending=False\n",
        ").take(5)\n",
        "\n",
        "for emp_id, emp, best_score in best_matches:\n",
        "    print(f\"\\n  {emp['user_name']} - Best Match Score: {best_score:.3f}\")\n",
        "    print(f\"    Skills: {', '.join(emp['skillset'][:3])}\")\n",
        "    print(f\"    Experience: {emp['experience_years']} yrs, \"\n",
        "          f\"CTC: {emp['ctc']} LPA, City: {emp['city']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Generate recommendations for all employees\n",
        "recommendations_rdd = employee_rdd.map(generate_recommendations)\n",
        "\n",
        "print(\"✓ Recommendations generated for all employees using RDD map operation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BhTrGB-0yqF",
        "outputId": "8940279b-dc4a-4e73-cbc1-3b4d3ff09732"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Task 4: Top 3 Recommended Jobs for Sample Employees (Ranked by Weight)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "Employee: Jose Ramirez (ID: 1)\n",
            "Current: Pune, CTC: 14.75 LPA, Exp: 6 yrs\n",
            "Skills: Python, Node.js, React\n",
            "\n",
            "Top 3 Recommended Jobs:\n",
            "\n",
            "  Rank 1 - Score: 1.000\n",
            "    Job: DevOps Engineer at Cognizant\n",
            "    Location: Pune, CTC: 16.25 LPA\n",
            "    Exp Required: 6 yrs\n",
            "    Required Skills: Angular, React\n",
            "    Skill Match: 100.0%\n",
            "\n",
            "  Rank 2 - Score: 1.000\n",
            "    Job: Software Engineer at Tech Mahindra\n",
            "    Location: Pune, CTC: 25.85 LPA\n",
            "    Exp Required: 6 yrs\n",
            "    Required Skills: Python, Angular\n",
            "    Skill Match: 100.0%\n",
            "\n",
            "  Rank 3 - Score: 0.985\n",
            "    Job: DevOps Engineer at HCL\n",
            "    Location: Pune, CTC: 18.21 LPA\n",
            "    Exp Required: 5 yrs\n",
            "    Required Skills: React, Node.js\n",
            "    Skill Match: 100.0%\n",
            "\n",
            "======================================================================\n",
            "Employee: Andrea Edwards (ID: 2)\n",
            "Current: Pune, CTC: 20.44 LPA, Exp: 5 yrs\n",
            "Skills: AWS, Deep Learning\n",
            "\n",
            "Top 3 Recommended Jobs:\n",
            "\n",
            "  Rank 1 - Score: 0.900\n",
            "    Job: DevOps Engineer at Cognizant\n",
            "    Location: Ahmedabad, CTC: 24.75 LPA\n",
            "    Exp Required: 5 yrs\n",
            "    Required Skills: AWS, Deep Learning\n",
            "    Skill Match: 100.0%\n",
            "\n",
            "  Rank 2 - Score: 0.840\n",
            "    Job: Data Engineer at Cognizant\n",
            "    Location: Hyderabad, CTC: 31.95 LPA\n",
            "    Exp Required: 1 yrs\n",
            "    Required Skills: Deep Learning, AWS\n",
            "    Skill Match: 100.0%\n",
            "\n",
            "  Rank 3 - Score: 0.837\n",
            "    Job: Product Manager at HCL\n",
            "    Location: Pune, CTC: 23.23 LPA\n",
            "    Exp Required: 3 yrs\n",
            "    Required Skills: AWS, Python, Deep Learning\n",
            "    Skill Match: 66.7%\n",
            "\n",
            "======================================================================\n",
            "Employee: Greg Hines (ID: 3)\n",
            "Current: Pune, CTC: 9.12 LPA, Exp: 1 yrs\n",
            "Skills: Angular, Deep Learning, Machine Learning\n",
            "\n",
            "Top 3 Recommended Jobs:\n",
            "\n",
            "  Rank 1 - Score: 1.000\n",
            "    Job: Full Stack Developer at Microsoft\n",
            "    Location: Pune, CTC: 18.49 LPA\n",
            "    Exp Required: 1 yrs\n",
            "    Required Skills: Deep Learning, Angular\n",
            "    Skill Match: 100.0%\n",
            "\n",
            "  Rank 2 - Score: 1.000\n",
            "    Job: Data Engineer at Accenture\n",
            "    Location: Pune, CTC: 15.92 LPA\n",
            "    Exp Required: 1 yrs\n",
            "    Required Skills: Machine Learning, Deep Learning\n",
            "    Skill Match: 100.0%\n",
            "\n",
            "  Rank 3 - Score: 0.985\n",
            "    Job: Cloud Architect at TCS\n",
            "    Location: Pune, CTC: 29.71 LPA\n",
            "    Exp Required: 0 yrs\n",
            "    Required Skills: AWS, Angular\n",
            "    Skill Match: 100.0%\n",
            "\n",
            "======================================================================\n",
            "Employee: Timothy Fisher (ID: 4)\n",
            "Current: Mumbai, CTC: 5.52 LPA, Exp: 1 yrs\n",
            "Skills: Kubernetes, Node.js, Data Science\n",
            "\n",
            "Top 3 Recommended Jobs:\n",
            "\n",
            "  Rank 1 - Score: 1.000\n",
            "    Job: Data Scientist at Microsoft\n",
            "    Location: Mumbai, CTC: 26.44 LPA\n",
            "    Exp Required: 1 yrs\n",
            "    Required Skills: Data Science, Kubernetes\n",
            "    Skill Match: 100.0%\n",
            "\n",
            "  Rank 2 - Score: 1.000\n",
            "    Job: DevOps Engineer at Amazon\n",
            "    Location: Mumbai, CTC: 22.02 LPA\n",
            "    Exp Required: 1 yrs\n",
            "    Required Skills: Node.js, Data Science, Kubernetes\n",
            "    Skill Match: 100.0%\n",
            "\n",
            "  Rank 3 - Score: 0.900\n",
            "    Job: Product Manager at Accenture\n",
            "    Location: Mumbai, CTC: 31.11 LPA\n",
            "    Exp Required: 1 yrs\n",
            "    Required Skills: Data Science, Python, React, Node.js\n",
            "    Skill Match: 75.0%\n",
            "\n",
            "======================================================================\n",
            "Employee: Henry Long (ID: 5)\n",
            "Current: Ahmedabad, CTC: 17.25 LPA, Exp: 9 yrs\n",
            "Skills: Python, Machine Learning, Computer Vision\n",
            "\n",
            "Top 3 Recommended Jobs:\n",
            "\n",
            "  Rank 1 - Score: 0.970\n",
            "    Job: Data Engineer at Infosys\n",
            "    Location: Ahmedabad, CTC: 32.66 LPA\n",
            "    Exp Required: 7 yrs\n",
            "    Required Skills: Node.js, Machine Learning, Angular\n",
            "    Skill Match: 100.0%\n",
            "\n",
            "  Rank 2 - Score: 0.955\n",
            "    Job: AI Researcher at Tech Mahindra\n",
            "    Location: Ahmedabad, CTC: 25.45 LPA\n",
            "    Exp Required: 6 yrs\n",
            "    Required Skills: Machine Learning, Angular\n",
            "    Skill Match: 100.0%\n",
            "\n",
            "  Rank 3 - Score: 0.954\n",
            "    Job: Frontend Developer at Infosys\n",
            "    Location: Ahmedabad, CTC: 17.2 LPA\n",
            "    Exp Required: 6 yrs\n",
            "    Required Skills: Machine Learning, Node.js\n",
            "    Skill Match: 100.0%\n",
            "\n",
            "================================================================================\n",
            "RECOMMENDATION STATISTICS\n",
            "================================================================================\n",
            "\n",
            "Total Employees: 1000\n",
            "Employees with Recommendations: 1000\n",
            "Average Recommendations per Employee: 3.00\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Employees with Best Job Matches\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  Jose Ramirez - Best Match Score: 1.000\n",
            "    Skills: Python, Node.js, React\n",
            "    Experience: 6 yrs, CTC: 14.75 LPA, City: Pune\n",
            "\n",
            "  Greg Hines - Best Match Score: 1.000\n",
            "    Skills: Angular, Deep Learning, Machine Learning\n",
            "    Experience: 1 yrs, CTC: 9.12 LPA, City: Pune\n",
            "\n",
            "  Timothy Fisher - Best Match Score: 1.000\n",
            "    Skills: Kubernetes, Node.js, Data Science\n",
            "    Experience: 1 yrs, CTC: 5.52 LPA, City: Mumbai\n",
            "\n",
            "  Melissa Carlson - Best Match Score: 1.000\n",
            "    Skills: Deep Learning, Node.js, Angular\n",
            "    Experience: 6 yrs, CTC: 22.66 LPA, City: Bangalore\n",
            "\n",
            "  Christy Estes - Best Match Score: 1.000\n",
            "    Skills: React, Node.js, Python\n",
            "    Experience: 5 yrs, CTC: 28.28 LPA, City: Chennai\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS COMPLETE\n",
            "================================================================================\n",
            "✓ Recommendations generated for all employees using RDD map operation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommendation System Statistics"
      ],
      "metadata": {
        "id": "dI1aFPh80-wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RECOMMENDATION SYSTEM STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "total_recommendations = recommendations_rdd.map(\n",
        "    lambda x: len(x[2])\n",
        ").reduce(lambda a, b: a + b)\n",
        "\n",
        "employees_with_recs = recommendations_rdd.filter(\n",
        "    lambda x: len(x[2]) > 0\n",
        ").count()\n",
        "\n",
        "employees_without_recs = recommendations_rdd.filter(\n",
        "    lambda x: len(x[2]) == 0\n",
        ").count()\n",
        "\n",
        "avg_recs = total_recommendations / employee_rdd.count()\n",
        "\n",
        "print(f\"\\nOVERALL METRICS:\")\n",
        "print(f\"  Total Employees Analyzed: {employee_rdd.count()}\")\n",
        "print(f\"  Employees with Recommendations: {employees_with_recs} ({employees_with_recs/employee_rdd.count()*100:.1f}%)\")\n",
        "print(f\"  Employees without Recommendations: {employees_without_recs} ({employees_without_recs/employee_rdd.count()*100:.1f}%)\")\n",
        "print(f\"  Total Recommendations Generated: {total_recommendations}\")\n",
        "print(f\"  Average Recommendations per Employee: {avg_recs:.2f}\")\n",
        "print(f\"  Total Job Postings Available: {job_rdd.count()}\")\n",
        "\n",
        "# Distribution of recommendation counts\n",
        "rec_distribution = recommendations_rdd.map(\n",
        "    lambda x: (len(x[2]), 1)\n",
        ").reduceByKey(lambda a, b: a + b).collect()\n",
        "\n",
        "print(f\"\\nRECOMMENDATION DISTRIBUTION:\")\n",
        "for num_recs, count in sorted(rec_distribution):\n",
        "    print(f\"  {num_recs} recommendations: {count} employees ({count/employee_rdd.count()*100:.1f}%)\")\n",
        "\n",
        "# Find employees with highest scoring recommendations\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TOP 5 EMPLOYEES WITH BEST JOB MATCHES\")\n",
        "print(\"-\"*80)\n",
        "print(\"(Employees whose top recommendation has the highest score)\\n\")\n",
        "\n",
        "best_matches = recommendations_rdd.filter(\n",
        "    lambda x: len(x[2]) > 0\n",
        ").map(\n",
        "    lambda x: (x[0], x[1], x[2][0][0], x[2][0][1])  # emp_id, emp, best_score, best_job\n",
        ").sortBy(\n",
        "    lambda x: x[2], ascending=False\n",
        ").take(5)\n",
        "\n",
        "for rank, (emp_id, emp, best_score, best_job) in enumerate(best_matches, 1):\n",
        "    print(f\"{rank}. {emp['user_name']} - Match Score: {best_score:.3f}\")\n",
        "    print(f\"   Employee: {', '.join(emp['skillset'][:3])}, {emp['experience_years']} yrs exp, {emp['city']}\")\n",
        "    print(f\"   Best Match: {best_job['job_role']} at {best_job['company_name']}\")\n",
        "    print(f\"   Job Offers: {best_job['ctc_expected']} LPA in {best_job['city']}\")\n",
        "    print()\n",
        "\n",
        "# Analyze score distribution\n",
        "all_scores = recommendations_rdd.flatMap(\n",
        "    lambda x: [rec[0] for rec in x[2]]\n",
        ").collect()\n",
        "\n",
        "if all_scores:\n",
        "    avg_score = sum(all_scores) / len(all_scores)\n",
        "    max_score = max(all_scores)\n",
        "    min_score = min(all_scores)\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(\"SCORE STATISTICS:\")\n",
        "    print(f\"  Average Recommendation Score: {avg_score:.3f}\")\n",
        "    print(f\"  Highest Recommendation Score: {max_score:.3f}\")\n",
        "    print(f\"  Lowest Recommendation Score: {min_score:.3f}\")\n",
        "\n",
        "    # Score ranges\n",
        "    excellent = len([s for s in all_scores if s >= 0.7])\n",
        "    good = len([s for s in all_scores if 0.5 <= s < 0.7])\n",
        "    fair = len([s for s in all_scores if 0.3 <= s < 0.5])\n",
        "    poor = len([s for s in all_scores if s < 0.3])\n",
        "\n",
        "    print(f\"\\nSCORE DISTRIBUTION:\")\n",
        "    print(f\"  Excellent (≥0.7): {excellent} recommendations ({excellent/len(all_scores)*100:.1f}%)\")\n",
        "    print(f\"  Good (0.5-0.7):   {good} recommendations ({good/len(all_scores)*100:.1f}%)\")\n",
        "    print(f\"  Fair (0.3-0.5):   {fair} recommendations ({fair/len(all_scores)*100:.1f}%)\")\n",
        "    print(f\"  Poor (<0.3):      {poor} recommendations ({poor/len(all_scores)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nKEY INSIGHTS:\")\n",
        "print(\"• Recommendation algorithm uses weighted scoring (Skillset 40%, Experience 30%,\")\n",
        "print(\"  CTC 20%, City 10%)\")\n",
        "print(\"• Each employee receives up to 3 job recommendations ranked by fit score\")\n",
        "print(\"• Higher scores indicate better overall match across all criteria\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuXu5qX11Bg5",
        "outputId": "9f2b95fe-c938-4c9c-da08-3f995f578de2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "RECOMMENDATION SYSTEM STATISTICS\n",
            "================================================================================\n",
            "\n",
            "OVERALL METRICS:\n",
            "  Total Employees Analyzed: 1000\n",
            "  Employees with Recommendations: 1000 (100.0%)\n",
            "  Employees without Recommendations: 0 (0.0%)\n",
            "  Total Recommendations Generated: 3000\n",
            "  Average Recommendations per Employee: 3.00\n",
            "  Total Job Postings Available: 5000\n",
            "\n",
            "RECOMMENDATION DISTRIBUTION:\n",
            "  3 recommendations: 1000 employees (100.0%)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TOP 5 EMPLOYEES WITH BEST JOB MATCHES\n",
            "--------------------------------------------------------------------------------\n",
            "(Employees whose top recommendation has the highest score)\n",
            "\n",
            "1. Jose Ramirez - Match Score: 1.000\n",
            "   Employee: Python, Node.js, React, 6 yrs exp, Pune\n",
            "   Best Match: DevOps Engineer at Cognizant\n",
            "   Job Offers: 16.25 LPA in Pune\n",
            "\n",
            "2. Greg Hines - Match Score: 1.000\n",
            "   Employee: Angular, Deep Learning, Machine Learning, 1 yrs exp, Pune\n",
            "   Best Match: Full Stack Developer at Microsoft\n",
            "   Job Offers: 18.49 LPA in Pune\n",
            "\n",
            "3. Timothy Fisher - Match Score: 1.000\n",
            "   Employee: Kubernetes, Node.js, Data Science, 1 yrs exp, Mumbai\n",
            "   Best Match: Data Scientist at Microsoft\n",
            "   Job Offers: 26.44 LPA in Mumbai\n",
            "\n",
            "4. Melissa Carlson - Match Score: 1.000\n",
            "   Employee: Deep Learning, Node.js, Angular, 6 yrs exp, Bangalore\n",
            "   Best Match: DevOps Engineer at Wipro\n",
            "   Job Offers: 32.52 LPA in Bangalore\n",
            "\n",
            "5. Christy Estes - Match Score: 1.000\n",
            "   Employee: React, Node.js, Python, 5 yrs exp, Chennai\n",
            "   Best Match: AI Researcher at Tech Mahindra\n",
            "   Job Offers: 28.41 LPA in Chennai\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SCORE STATISTICS:\n",
            "  Average Recommendation Score: 0.913\n",
            "  Highest Recommendation Score: 1.000\n",
            "  Lowest Recommendation Score: 0.700\n",
            "\n",
            "SCORE DISTRIBUTION:\n",
            "  Excellent (≥0.7): 3000 recommendations (100.0%)\n",
            "  Good (0.5-0.7):   0 recommendations (0.0%)\n",
            "  Fair (0.3-0.5):   0 recommendations (0.0%)\n",
            "  Poor (<0.3):      0 recommendations (0.0%)\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS COMPLETE\n",
            "================================================================================\n",
            "\n",
            "KEY INSIGHTS:\n",
            "• Recommendation algorithm uses weighted scoring (Skillset 40%, Experience 30%,\n",
            "  CTC 20%, City 10%)\n",
            "• Each employee receives up to 3 job recommendations ranked by fit score\n",
            "• Higher scores indicate better overall match across all criteria\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}