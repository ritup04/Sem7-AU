{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d3a88cb5",
      "metadata": {
        "id": "d3a88cb5"
      },
      "source": [
        "# Model Fitting & Optimization from Scratch (NumPy)\n",
        "\n",
        "**Goal:** Implement a multi-layer neural network *from scratch* using NumPy to learn classifiers on MNIST and CIFAR. This Colab notebook guides students through writing activations, forward/backward passes, loss functions (cross-entropy, Huber/L1/L2), and implementing SGD with momentum and weight decay.\n",
        "\n",
        "---\n",
        "\n",
        "**Notes for instructors:**\n",
        "- This notebook is structured with explanation cells and code cells. Students should fill the `TODO` sections.\n",
        "- Designed to run in Google Colab. For CIFAR training you'll likely need GPU runtime to speed up.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c197577a",
      "metadata": {
        "id": "c197577a"
      },
      "source": [
        "## 1) Setup & Imports\n",
        "\n",
        "Run the cell below to import libraries. If running in a fresh Colab environment, TensorFlow is available by default to load datasets. This notebook uses NumPy only for model math (no autograd)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e79802d",
      "metadata": {
        "id": "9e79802d"
      },
      "outputs": [],
      "source": [
        "# Standard imports - this code is ready for Colab\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "# For dataset loading (Colab has tensorflow installed)\n",
        "try:\n",
        "    from tensorflow.keras.datasets import mnist, cifar10\n",
        "    tf_available = True\n",
        "except Exception as e:\n",
        "    print(\"TensorFlow dataset loader not available. You can still load datasets manually.\")\n",
        "    tf_available = False\n",
        "\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd2eca87",
      "metadata": {
        "id": "fd2eca87"
      },
      "source": [
        "## 2) Utilities\n",
        "\n",
        "Helper functions for one-hot encoding, stable softmax, accuracy and plotting training curves.\n",
        "\n",
        "\n",
        "1.  One-hot encoding\tConvert integer labels to vector form for loss computation\n",
        "\n",
        "\n",
        "2.   Stable Softmax\tSafely convert logits to probabilities without overflow\n",
        "\n",
        "\n",
        "\n",
        "1.   Accuracy\tMeasure model correctness\n",
        "2.   Plot training curves\tVisualize learning progress & detect over/underfitting\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03755950",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03755950",
        "outputId": "a10c2106-37f2-41b7-d091-97b321765f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utilities loaded.\n"
          ]
        }
      ],
      "source": [
        "def one_hot(labels: np.ndarray, num_classes: int) -> np.ndarray:\n",
        "    y = np.zeros((labels.shape[0], num_classes), dtype=np.float32)\n",
        "    for i, lab in enumerate(labels):\n",
        "        y[i, int(lab)] = 1.0\n",
        "    return y\n",
        "\n",
        "def stable_softmax(x: np.ndarray) -> np.ndarray:\n",
        "    # x: (batch, classes)\n",
        "    x_max = np.max(x, axis=1, keepdims=True)\n",
        "    ex = np.exp(x - x_max)\n",
        "    return ex / np.sum(ex, axis=1, keepdims=True)\n",
        "\n",
        "def accuracy(pred_probs: np.ndarray, labels: np.ndarray) -> float:\n",
        "    preds = np.argmax(pred_probs, axis=1)\n",
        "    return np.mean(preds == labels) * 100.0\n",
        "\n",
        "def plot_training(history: Dict[str, List[float]]):\n",
        "    epochs = len(history['train_loss'])\n",
        "    fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
        "    ax[0].plot(range(1, epochs+1), history['train_loss'], label='train loss')\n",
        "    ax[0].plot(range(1, epochs+1), history.get('val_loss', []), label='val loss')\n",
        "    ax[0].set_xlabel('Epoch'); ax[0].set_ylabel('Loss'); ax[0].legend()\n",
        "    ax[1].plot(range(1, epochs+1), history['train_acc'], label='train acc')\n",
        "    ax[1].plot(range(1, epochs+1), history.get('val_acc', []), label='val acc')\n",
        "    ax[1].set_xlabel('Epoch'); ax[1].set_ylabel('Accuracy (%)'); ax[1].legend()\n",
        "    plt.show()\n",
        "\n",
        "print('Utilities loaded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abf5d8f1",
      "metadata": {
        "id": "abf5d8f1"
      },
      "source": [
        "## 3) Activations & Derivatives\n",
        "\n",
        "Implement activations and their derivatives. Softmax is handled in the final layer; for backprop we provide a Jacobian helper for educational clarity (students may use alternative vectorized approaches).\n",
        "Educational Notes for Students\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "Squashes values to (0, 1) range.\n",
        "\n",
        "Good for probabilities but suffers from vanishing gradients for large |x|.\n",
        "\n",
        "Derivative:\n",
        "ðœŽ\n",
        "â€²\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "ðœŽ\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "(\n",
        "1\n",
        "âˆ’\n",
        "ðœŽ\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        ")\n",
        "Ïƒ\n",
        "â€²\n",
        "(x)=Ïƒ(x)(1âˆ’Ïƒ(x))\n",
        "\n",
        "Tanh\n",
        "\n",
        "Output in (-1, 1).\n",
        "\n",
        "Centered, but still has vanishing gradient issues.\n",
        "\n",
        "Derivative:\n",
        "1\n",
        "âˆ’\n",
        "tanh\n",
        "â¡\n",
        "2\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "1âˆ’tanh\n",
        "2\n",
        "(x)\n",
        "\n",
        "ReLU\n",
        "\n",
        "Zero for negative inputs, identity for positive.\n",
        "\n",
        "Fast convergence but can cause \"dead neurons\".\n",
        "\n",
        "Derivative: 0 if\n",
        "ð‘¥\n",
        "â‰¤\n",
        "0\n",
        "xâ‰¤0, else 1.\n",
        "\n",
        "Leaky ReLU\n",
        "\n",
        "Fixes dead neuron issue by giving small slope (Î±) for negatives.\n",
        "\n",
        "Softmax\n",
        "\n",
        "Converts logits to probabilities for multi-class classification.\n",
        "\n",
        "In backprop, for cross-entropy + softmax, derivative simplifies to:\n",
        "\n",
        "âˆ‚\n",
        "ð¿\n",
        "âˆ‚\n",
        "ð‘§\n",
        "=\n",
        "softmax\n",
        "(\n",
        "ð‘§\n",
        ")\n",
        "âˆ’\n",
        "ð‘¦\n",
        "one-hot\n",
        "âˆ‚z\n",
        "âˆ‚L\n",
        "\tâ€‹\n",
        "\n",
        "=softmax(z)âˆ’y\n",
        "one-hot\n",
        "\tâ€‹\n",
        "\n",
        "\n",
        "so in practice you donâ€™t need the full Jacobian."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10274d2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10274d2d",
        "outputId": "801874b6-1349-4562-c3d6-767507c64589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Activations ready.\n"
          ]
        }
      ],
      "source": [
        "def linear(x): return x\n",
        "def d_linear(out): return np.ones_like(out)\n",
        "\n",
        "def sigmoid(x): return 1.0 / (1.0 + np.exp(-x))\n",
        "def d_sigmoid(out): return out * (1.0 - out)\n",
        "\n",
        "def tanh_act(x): return np.tanh(x)\n",
        "def d_tanh(out): return 1.0 - out**2\n",
        "\n",
        "def relu(x): return np.maximum(0, x)\n",
        "def d_relu(out): return (out > 0).astype(np.float32)\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x >= 0, x, alpha * x)\n",
        "def d_leaky_relu_out(out, alpha=0.01):\n",
        "    # out is activated output; derivative needs original x, but we can reconstruct sign from out for leaky relu\n",
        "    # For positive outputs derivative is 1, for negative it's alpha (approximately)\n",
        "    deriv = np.ones_like(out)\n",
        "    deriv[out < 0] = alpha\n",
        "    return deriv\n",
        "\n",
        "# Softmax forward is stable_softmax above.\n",
        "# For backward, we'll provide a Jacobian computation for a single data point (educational)\n",
        "def softmax_jacobian(s: np.ndarray) -> np.ndarray:\n",
        "    # s is 1D softmax output vector (C,)\n",
        "    s = s.reshape(-1,1)\n",
        "    return np.diagflat(s) - s.dot(s.T)\n",
        "\n",
        "def d_softmax(prev_grad: np.ndarray, softmax_out: np.ndarray) -> np.ndarray:\n",
        "    # prev_grad: (batch, classes) gradient coming from next layer (dL/dy)\n",
        "    # softmax_out: (batch, classes)\n",
        "    # Compute dL/dz where z is pre-softmax inputs.\n",
        "    batch = prev_grad.shape[0]\n",
        "    out = np.zeros_like(prev_grad)\n",
        "    for i in range(batch):\n",
        "        J = softmax_jacobian(softmax_out[i])\n",
        "        out[i] = prev_grad[i].dot(J)\n",
        "    return out\n",
        "\n",
        "# Activation dispatcher\n",
        "ACT_FNS = {\n",
        "    'linear': (linear, d_linear),\n",
        "    'sigmoid': (sigmoid, d_sigmoid),\n",
        "    'tanh': (tanh_act, d_tanh),\n",
        "    'relu': (relu, d_relu),\n",
        "    'leaky_relu': (leaky_relu, d_leaky_relu_out),\n",
        "}\n",
        "\n",
        "print('Activations ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d170fc6",
      "metadata": {
        "id": "7d170fc6"
      },
      "source": [
        "## 4) Loss Functions\n",
        "\n",
        "Implement cross-entropy (with softmax), L1, L2, and Huber losses and their derivatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f618940",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f618940",
        "outputId": "5a961e0f-da58-4516-cb3a-bcdf8e07caad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss functions ready.\n"
          ]
        }
      ],
      "source": [
        "def cross_entropy_loss(probs: np.ndarray, targets: np.ndarray) -> float:\n",
        "    # probs: (batch, classes) from softmax, targets: (batch, classes) one-hot\n",
        "    # small epsilon for numerical stability\n",
        "    eps = 1e-12\n",
        "    probs_clipped = np.clip(probs, eps, 1. - eps)\n",
        "    loss = -np.sum(targets * np.log(probs_clipped)) / probs.shape[0]\n",
        "    return loss\n",
        "\n",
        "def d_cross_entropy(probs: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
        "    # derivative dL/dz where z is pre-softmax inputs if probs = softmax(z)\n",
        "    # For cross-entropy with softmax, derivative simplifies to (probs - targets)/batch\n",
        "    batch = probs.shape[0]\n",
        "    return (probs - targets) / batch\n",
        "\n",
        "def l2_loss(pred: np.ndarray, target: np.ndarray):\n",
        "    diff = pred - target\n",
        "    return 0.5 * np.mean(diff**2)\n",
        "\n",
        "def d_l2(pred: np.ndarray, target: np.ndarray):\n",
        "    batch = pred.shape[0]\n",
        "    return (pred - target) / batch\n",
        "\n",
        "def l1_loss(pred: np.ndarray, target: np.ndarray):\n",
        "    return np.mean(np.abs(pred - target))\n",
        "\n",
        "def d_l1(pred: np.ndarray, target: np.ndarray):\n",
        "    diff = pred - target\n",
        "    batch = pred.shape[0]\n",
        "    # subgradient: sign(diff)\n",
        "    return np.sign(diff) / batch\n",
        "\n",
        "def huber_loss(pred: np.ndarray, target: np.ndarray, delta=1.0):\n",
        "    diff = pred - target\n",
        "    absd = np.abs(diff)\n",
        "    mask = absd <= delta\n",
        "    loss = np.where(mask, 0.5 * diff**2, delta * (absd - 0.5 * delta))\n",
        "    return np.mean(loss)\n",
        "\n",
        "def d_huber(pred: np.ndarray, target: np.ndarray, delta=1.0):\n",
        "    diff = pred - target\n",
        "    absd = np.abs(diff)\n",
        "    mask = absd <= delta\n",
        "    grad = np.where(mask, diff, delta * np.sign(diff))\n",
        "    batch = pred.shape[0]\n",
        "    return grad / batch\n",
        "\n",
        "print('Loss functions ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f80d8ee3",
      "metadata": {
        "id": "f80d8ee3"
      },
      "source": [
        "## 5) Layer & Model Classes\n",
        "\n",
        "Implement a simple Dense layer and Model class to handle forward/backward and parameter updates. Weight initialization options: Xavier (Glorot) and He."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "981d6253",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "981d6253",
        "outputId": "e17fc3b0-c4f3-4150-eda1-d0eba41a173f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer and Model classes defined.\n"
          ]
        }
      ],
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, input_dim:int, output_dim:int, activation='relu', init='xavier'):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.activation_name = activation\n",
        "        self.activation, self.d_activation = ACT_FNS.get(activation, ACT_FNS['relu'])\n",
        "\n",
        "        # weight init\n",
        "        if init == 'xavier':\n",
        "            limit = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "            self.W = np.random.uniform(-limit, limit, size=(input_dim, output_dim)).astype(np.float32)\n",
        "        elif init == 'he':\n",
        "            std = np.sqrt(2.0 / input_dim)\n",
        "            self.W = np.random.randn(input_dim, output_dim).astype(np.float32) * std\n",
        "        else:\n",
        "            self.W = np.random.randn(input_dim, output_dim).astype(np.float32) * 0.01\n",
        "\n",
        "        self.b = np.zeros((1, output_dim), dtype=np.float32)\n",
        "        # gradients and velocity for momentum\n",
        "        self.dW = np.zeros_like(self.W)\n",
        "        self.db = np.zeros_like(self.b)\n",
        "        self.vW = np.zeros_like(self.W)\n",
        "        self.vb = np.zeros_like(self.b)\n",
        "\n",
        "        # caches for backprop\n",
        "        self.x = None\n",
        "        self.z = None\n",
        "        self.a = None\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        # x: (batch, input_dim)\n",
        "        self.x = x\n",
        "        self.z = x.dot(self.W) + self.b  # (batch, output_dim)\n",
        "        # apply activation (special-case softmax handled by Model)\n",
        "        if self.activation_name == 'softmax':\n",
        "            self.a = stable_softmax(self.z)\n",
        "        else:\n",
        "            # activation functions expect pre-activation input; some were implemented to accept x\n",
        "            fn = ACT_FNS[self.activation_name][0]\n",
        "            self.a = fn(self.z)\n",
        "        return self.a\n",
        "\n",
        "    def backward(self, grad_a: np.ndarray) -> np.ndarray:\n",
        "        # grad_a: dL/da (batch, output_dim)\n",
        "        # compute dL/dz first\n",
        "        if self.activation_name == 'softmax':\n",
        "            # use simplified derivative if combined with cross-entropy outside (handled elsewhere)\n",
        "            # Otherwise use Jacobian-based derivative\n",
        "            grad_z = d_softmax(grad_a, self.a)\n",
        "        else:\n",
        "            # derivative function expects activated output or z depending on implementation; we use a\n",
        "            dfn = ACT_FNS[self.activation_name][1]\n",
        "            grad_z = grad_a * dfn(self.a)\n",
        "\n",
        "        # gradients w.r.t weights and bias\n",
        "        batch = self.x.shape[0]\n",
        "        self.dW = self.x.T.dot(grad_z)  # (input_dim, output_dim)\n",
        "        self.db = np.sum(grad_z, axis=0, keepdims=True)\n",
        "        # gradient w.r.t input to propagate to previous layer\n",
        "        grad_x = grad_z.dot(self.W.T)\n",
        "        return grad_x\n",
        "\n",
        "class SimpleModel:\n",
        "    def __init__(self, layers: List[DenseLayer]):\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        out = x\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            out = layer.forward(out)\n",
        "        return out\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> None:\n",
        "        grad = grad_output\n",
        "        for layer in reversed(self.layers):\n",
        "            grad = layer.backward(grad)\n",
        "\n",
        "    def update(self, lr: float, momentum: float=0.9, decay: float=0.0):\n",
        "        for layer in self.layers:\n",
        "            # velocity update: v = m*v - (dW + decay*W)\n",
        "            layer.vW = momentum * layer.vW - (layer.dW + decay * layer.W)\n",
        "            layer.vb = momentum * layer.vb - (layer.db + decay * layer.b)\n",
        "            # weights update: w += lr * v\n",
        "            layer.W += lr * layer.vW * lr  # note: multiply by lr twice is a bug; we will correct below in comment\n",
        "            layer.b += lr * layer.vb * lr\n",
        "\n",
        "    def correct_update(self, lr: float, momentum: float=0.9, decay: float=0.0):\n",
        "        # Correct implementation for students to use\n",
        "        for layer in self.layers:\n",
        "            layer.vW = momentum * layer.vW - lr * (layer.dW + decay * layer.W)\n",
        "            layer.vb = momentum * layer.vb - lr * (layer.db + decay * layer.b)\n",
        "            layer.W += layer.vW\n",
        "            layer.b += layer.vb\n",
        "\n",
        "print('Layer and Model classes defined.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2d0bd9c",
      "metadata": {
        "id": "a2d0bd9c"
      },
      "source": [
        "## 6) Training Loop\n",
        "\n",
        "Fill in or run the training loop. We provide a skeleton: forward, compute loss, backward, update. Use `correct_update` in practice. For MNIST quick runs use small network and fewer epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33656f16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33656f16",
        "outputId": "30b18a80-f9f9-455e-db36-0f7602a66428"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loop skeleton ready.\n"
          ]
        }
      ],
      "source": [
        "def train_model(model: SimpleModel,\n",
        "                X_train: np.ndarray, y_train: np.ndarray,\n",
        "                X_val: np.ndarray, y_val: np.ndarray,\n",
        "                epochs: int = 10, batch_size: int = 64,\n",
        "                lr: float = 0.01, momentum: float = 0.9, decay: float = 0.0,\n",
        "                loss_name: str = 'cross_entropy', verbose: bool = True):\n",
        "\n",
        "    n = X_train.shape[0]\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        perm = np.random.permutation(n)\n",
        "        X_sh = X_train[perm]\n",
        "        y_sh = y_train[perm]\n",
        "        epoch_loss = 0.0\n",
        "        epoch_acc = 0.0\n",
        "        t0 = time()\n",
        "        for i in range(0, n, batch_size):\n",
        "            xb = X_sh[i:i+batch_size]\n",
        "            yb = y_sh[i:i+batch_size]\n",
        "            # Forward\n",
        "            preds = model.forward(xb)  # if final layer is softmax this is probs\n",
        "            # Loss and gradient\n",
        "            if loss_name == 'cross_entropy':\n",
        "                loss = cross_entropy_loss(preds, yb)\n",
        "                grad = d_cross_entropy(preds, yb)  # dL/dz when softmax final\n",
        "            elif loss_name == 'l2':\n",
        "                loss = l2_loss(preds, yb)\n",
        "                grad = d_l2(preds, yb)\n",
        "            elif loss_name == 'l1':\n",
        "                loss = l1_loss(preds, yb)\n",
        "                grad = d_l1(preds, yb)\n",
        "            elif loss_name == 'huber':\n",
        "                loss = huber_loss(preds, yb)\n",
        "                grad = d_huber(preds, yb)\n",
        "            else:\n",
        "                raise ValueError('Unknown loss')\n",
        "\n",
        "            epoch_loss += loss * xb.shape[0]\n",
        "            epoch_acc += accuracy(preds, np.argmax(yb, axis=1)) * xb.shape[0]\n",
        "\n",
        "            # Backprop\n",
        "            model.backward(grad)\n",
        "            # Update weights\n",
        "            model.correct_update(lr, momentum, decay)\n",
        "\n",
        "        epoch_loss /= n\n",
        "        epoch_acc /= n\n",
        "\n",
        "        # Validation\n",
        "        val_preds = model.forward(X_val)\n",
        "        if loss_name == 'cross_entropy':\n",
        "            val_loss = cross_entropy_loss(val_preds, y_val)\n",
        "        else:\n",
        "            val_loss = 0.0\n",
        "        val_acc = accuracy(val_preds, np.argmax(y_val, axis=1))\n",
        "\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_acc'].append(epoch_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Epoch {ep}/{epochs} - loss: {epoch_loss:.4f} - acc: {epoch_acc:.2f}% - val_loss: {val_loss:.4f} - val_acc: {val_acc:.2f}% - time: {time()-t0:.1f}s')\n",
        "\n",
        "    return history\n",
        "\n",
        "print('Training loop skeleton ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea454e9",
      "metadata": {
        "id": "3ea454e9"
      },
      "source": [
        "## 7) Demo: MNIST Quick Setup\n",
        "\n",
        "Load MNIST, preprocess, build a tiny model to test the pipeline. This cell is ready to run in Colab. For speed during debugging, use a subset of the data (e.g., 5000 train samples)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f23f83c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f23f83c",
        "outputId": "4ffa499e-97cd-4cfe-fbac-96cccb9aac0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "MNIST loaded: (55000, 784) (5000, 784) (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "if tf_available:\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    # flatten and normalize\n",
        "    X_train = X_train.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
        "    X_test  = X_test.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
        "    # quick validation split\n",
        "    X_val = X_train[-5000:]; y_val = y_train[-5000:]\n",
        "    X_train = X_train[:-5000]; y_train = y_train[:-5000]\n",
        "    # one hot encode\n",
        "    y_train_oh = one_hot(y_train, 10)\n",
        "    y_val_oh   = one_hot(y_val, 10)\n",
        "    y_test_oh  = one_hot(y_test, 10)\n",
        "    print('MNIST loaded:', X_train.shape, X_val.shape, X_test.shape)\n",
        "else:\n",
        "    print('TensorFlow datasets not available in this environment.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec79152",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bec79152",
        "outputId": "1c47ca11-b218-42df-a808-49be6e16436c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model ready. Run training with train_model(...)\n"
          ]
        }
      ],
      "source": [
        "# Example model creation (to run in Colab)\n",
        "# Create a small model: 784 -> 128(relu) -> 10(softmax)\n",
        "# NOTE: For softmax final layer, we set activation_name to 'softmax' manually.\n",
        "input_dim = 784\n",
        "hidden = DenseLayer(input_dim, 128, activation='relu', init='he')\n",
        "output = DenseLayer(128, 10, activation='softmax', init='xavier')\n",
        "\n",
        "model = SimpleModel([hidden, output])\n",
        "print('Model ready. Run training with train_model(...)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2453be90",
      "metadata": {
        "id": "2453be90"
      },
      "source": [
        "## 8) Experiments & Hyperparameter Tuning\n",
        "\n",
        "Try these experiments:\n",
        "- Vary learning rate: [1e-3, 1e-2, 1e-1]\n",
        "- Momentum: [0.0, 0.9, 0.99]\n",
        "- Weight decay: [0.0, 1e-4, 1e-3]\n",
        "- Activations: relu vs tanh vs sigmoid\n",
        "- Loss functions: cross-entropy vs huber\n",
        "\n",
        "Record training curves and final test accuracy. Try CIFAR after confirming MNIST works. For CIFAR expect to tune lr and use smaller network or more epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56-_AgtP1603",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "56-_AgtP1603",
        "outputId": "f312b4a0-0798-4ca3-b5cd-8c6b74ec952f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNIST loaded: (55000, 784) (5000, 784) (10000, 784)\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.0, act=relu, loss=cross_entropy\n",
            "Val acc=38.66% | Test acc=37.36%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.0, act=relu, loss=huber\n",
            "Val acc=49.40% | Test acc=49.48%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.0, act=tanh, loss=cross_entropy\n",
            "Val acc=54.00% | Test acc=52.52%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.0, act=tanh, loss=huber\n",
            "Val acc=50.48% | Test acc=50.16%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.0, act=sigmoid, loss=cross_entropy\n",
            "Val acc=20.40% | Test acc=18.80%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.0, act=sigmoid, loss=huber\n",
            "Val acc=19.08% | Test acc=19.14%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.0001, act=relu, loss=cross_entropy\n",
            "Val acc=50.22% | Test acc=46.99%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.0001, act=relu, loss=huber\n",
            "Val acc=45.96% | Test acc=44.21%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.0001, act=tanh, loss=cross_entropy\n",
            "Val acc=61.18% | Test acc=58.25%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.0001, act=tanh, loss=huber\n",
            "Val acc=52.18% | Test acc=50.53%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.0001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=26.40% | Test acc=27.97%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.0001, act=sigmoid, loss=huber\n",
            "Val acc=29.94% | Test acc=27.00%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.001, act=relu, loss=cross_entropy\n",
            "Val acc=42.16% | Test acc=41.44%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.001, act=relu, loss=huber\n",
            "Val acc=49.94% | Test acc=48.28%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.001, act=tanh, loss=cross_entropy\n",
            "Val acc=51.20% | Test acc=48.99%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.001, act=tanh, loss=huber\n",
            "Val acc=55.82% | Test acc=54.59%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=21.26% | Test acc=21.99%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.0, wd=0.001, act=sigmoid, loss=huber\n",
            "Val acc=23.62% | Test acc=23.35%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.0, act=relu, loss=cross_entropy\n",
            "Val acc=90.28% | Test acc=88.20%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.0, act=relu, loss=huber\n",
            "Val acc=90.58% | Test acc=88.53%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.0, act=tanh, loss=cross_entropy\n",
            "Val acc=90.92% | Test acc=88.60%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.0, act=tanh, loss=huber\n",
            "Val acc=91.22% | Test acc=88.71%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.0, act=sigmoid, loss=cross_entropy\n",
            "Val acc=63.42% | Test acc=61.35%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.0, act=sigmoid, loss=huber\n",
            "Val acc=61.02% | Test acc=58.29%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.0001, act=relu, loss=cross_entropy\n",
            "Val acc=90.38% | Test acc=88.42%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.0001, act=relu, loss=huber\n",
            "Val acc=90.24% | Test acc=88.36%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.0001, act=tanh, loss=cross_entropy\n",
            "Val acc=90.58% | Test acc=88.64%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.0001, act=tanh, loss=huber\n",
            "Val acc=90.56% | Test acc=88.26%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.0001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=55.32% | Test acc=53.50%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.0001, act=sigmoid, loss=huber\n",
            "Val acc=57.68% | Test acc=56.24%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.001, act=relu, loss=cross_entropy\n",
            "Val acc=90.74% | Test acc=88.55%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.001, act=relu, loss=huber\n",
            "Val acc=90.32% | Test acc=88.80%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.001, act=tanh, loss=cross_entropy\n",
            "Val acc=90.68% | Test acc=88.70%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.001, act=tanh, loss=huber\n",
            "Val acc=90.12% | Test acc=88.23%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=54.58% | Test acc=52.23%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.9, wd=0.001, act=sigmoid, loss=huber\n",
            "Val acc=52.48% | Test acc=51.05%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.0, act=relu, loss=cross_entropy\n",
            "Val acc=95.18% | Test acc=93.81%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.0, act=relu, loss=huber\n",
            "Val acc=95.40% | Test acc=93.93%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.0, act=tanh, loss=cross_entropy\n",
            "Val acc=94.46% | Test acc=93.13%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.0, act=tanh, loss=huber\n",
            "Val acc=94.60% | Test acc=93.06%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.0, act=sigmoid, loss=cross_entropy\n",
            "Val acc=91.96% | Test acc=90.09%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.0, act=sigmoid, loss=huber\n",
            "Val acc=91.86% | Test acc=90.20%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.0001, act=relu, loss=cross_entropy\n",
            "Val acc=95.24% | Test acc=93.93%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.0001, act=relu, loss=huber\n",
            "Val acc=95.00% | Test acc=93.74%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.0001, act=tanh, loss=cross_entropy\n",
            "Val acc=94.42% | Test acc=92.89%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.0001, act=tanh, loss=huber\n",
            "Val acc=94.62% | Test acc=93.20%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.0001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=91.94% | Test acc=90.30%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.0001, act=sigmoid, loss=huber\n",
            "Val acc=91.80% | Test acc=90.06%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.001, act=relu, loss=cross_entropy\n",
            "Val acc=95.02% | Test acc=93.50%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.001, act=relu, loss=huber\n",
            "Val acc=94.80% | Test acc=93.53%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.001, act=tanh, loss=cross_entropy\n",
            "Val acc=93.98% | Test acc=92.60%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.001, act=tanh, loss=huber\n",
            "Val acc=94.14% | Test acc=92.78%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=91.54% | Test acc=89.90%\n",
            "\n",
            ">>> Training with lr=0.001, mom=0.99, wd=0.001, act=sigmoid, loss=huber\n",
            "Val acc=91.80% | Test acc=89.91%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.0, act=relu, loss=cross_entropy\n",
            "Val acc=90.94% | Test acc=88.30%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.0, act=relu, loss=huber\n",
            "Val acc=90.80% | Test acc=88.84%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.0, act=tanh, loss=cross_entropy\n",
            "Val acc=90.46% | Test acc=88.45%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.0, act=tanh, loss=huber\n",
            "Val acc=90.46% | Test acc=88.45%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.0, act=sigmoid, loss=cross_entropy\n",
            "Val acc=63.80% | Test acc=61.60%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.0, act=sigmoid, loss=huber\n",
            "Val acc=59.86% | Test acc=56.79%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.0001, act=relu, loss=cross_entropy\n",
            "Val acc=90.44% | Test acc=88.36%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.0001, act=relu, loss=huber\n",
            "Val acc=90.64% | Test acc=88.29%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.0001, act=tanh, loss=cross_entropy\n",
            "Val acc=90.58% | Test acc=88.68%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.0001, act=tanh, loss=huber\n",
            "Val acc=90.56% | Test acc=88.39%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.0001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=58.62% | Test acc=55.91%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.0001, act=sigmoid, loss=huber\n",
            "Val acc=61.12% | Test acc=58.96%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.001, act=relu, loss=cross_entropy\n",
            "Val acc=90.56% | Test acc=88.35%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.001, act=relu, loss=huber\n",
            "Val acc=90.26% | Test acc=88.27%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.001, act=tanh, loss=cross_entropy\n",
            "Val acc=90.38% | Test acc=88.51%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.001, act=tanh, loss=huber\n",
            "Val acc=90.56% | Test acc=88.60%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=56.46% | Test acc=53.72%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.0, wd=0.001, act=sigmoid, loss=huber\n",
            "Val acc=58.20% | Test acc=55.97%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.0, act=relu, loss=cross_entropy\n",
            "Val acc=95.42% | Test acc=93.93%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.0, act=relu, loss=huber\n",
            "Val acc=95.46% | Test acc=94.09%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.0, act=tanh, loss=cross_entropy\n",
            "Val acc=94.48% | Test acc=93.28%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.0, act=tanh, loss=huber\n",
            "Val acc=94.68% | Test acc=93.23%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.0, act=sigmoid, loss=cross_entropy\n",
            "Val acc=92.04% | Test acc=90.13%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.0, act=sigmoid, loss=huber\n",
            "Val acc=92.22% | Test acc=90.47%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.0001, act=relu, loss=cross_entropy\n",
            "Val acc=95.42% | Test acc=94.18%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.0001, act=relu, loss=huber\n",
            "Val acc=95.56% | Test acc=93.93%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.0001, act=tanh, loss=cross_entropy\n",
            "Val acc=94.22% | Test acc=93.00%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.0001, act=tanh, loss=huber\n",
            "Val acc=94.60% | Test acc=93.06%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.0001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=92.04% | Test acc=90.40%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.0001, act=sigmoid, loss=huber\n",
            "Val acc=91.86% | Test acc=90.31%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.001, act=relu, loss=cross_entropy\n",
            "Val acc=95.12% | Test acc=93.95%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.001, act=relu, loss=huber\n",
            "Val acc=94.94% | Test acc=93.64%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.001, act=tanh, loss=cross_entropy\n",
            "Val acc=94.00% | Test acc=92.41%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.001, act=tanh, loss=huber\n",
            "Val acc=93.96% | Test acc=92.53%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=91.82% | Test acc=89.78%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.9, wd=0.001, act=sigmoid, loss=huber\n",
            "Val acc=91.72% | Test acc=89.84%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.0, act=relu, loss=cross_entropy\n",
            "Val acc=97.94% | Test acc=97.22%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.0, act=relu, loss=huber\n",
            "Val acc=97.38% | Test acc=97.12%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.0, act=tanh, loss=cross_entropy\n",
            "Val acc=97.50% | Test acc=96.80%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.0, act=tanh, loss=huber\n",
            "Val acc=97.68% | Test acc=96.70%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.0, act=sigmoid, loss=cross_entropy\n",
            "Val acc=87.68% | Test acc=86.98%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.0, act=sigmoid, loss=huber\n",
            "Val acc=96.06% | Test acc=94.63%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.0001, act=relu, loss=cross_entropy\n",
            "Val acc=97.60% | Test acc=96.85%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.0001, act=relu, loss=huber\n",
            "Val acc=97.68% | Test acc=97.16%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.0001, act=tanh, loss=cross_entropy\n",
            "Val acc=97.36% | Test acc=96.82%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.0001, act=tanh, loss=huber\n",
            "Val acc=97.16% | Test acc=96.73%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.0001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=95.28% | Test acc=93.68%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.0001, act=sigmoid, loss=huber\n",
            "Val acc=87.50% | Test acc=86.61%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.001, act=relu, loss=cross_entropy\n",
            "Val acc=96.84% | Test acc=95.94%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.001, act=relu, loss=huber\n",
            "Val acc=97.10% | Test acc=95.94%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.001, act=tanh, loss=cross_entropy\n",
            "Val acc=96.14% | Test acc=94.91%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.001, act=tanh, loss=huber\n",
            "Val acc=96.04% | Test acc=94.81%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=92.18% | Test acc=90.35%\n",
            "\n",
            ">>> Training with lr=0.01, mom=0.99, wd=0.001, act=sigmoid, loss=huber\n",
            "Val acc=92.68% | Test acc=90.62%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.0, act=relu, loss=cross_entropy\n",
            "Val acc=95.36% | Test acc=94.24%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.0, act=relu, loss=huber\n",
            "Val acc=95.64% | Test acc=93.99%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.0, act=tanh, loss=cross_entropy\n",
            "Val acc=94.52% | Test acc=93.15%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.0, act=tanh, loss=huber\n",
            "Val acc=94.42% | Test acc=93.15%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.0, act=sigmoid, loss=cross_entropy\n",
            "Val acc=91.96% | Test acc=90.51%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.0, act=sigmoid, loss=huber\n",
            "Val acc=92.04% | Test acc=90.25%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.0001, act=relu, loss=cross_entropy\n",
            "Val acc=95.32% | Test acc=94.07%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.0001, act=relu, loss=huber\n",
            "Val acc=95.84% | Test acc=93.90%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.0001, act=tanh, loss=cross_entropy\n",
            "Val acc=94.58% | Test acc=93.14%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.0001, act=tanh, loss=huber\n",
            "Val acc=94.40% | Test acc=92.97%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.0001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=91.90% | Test acc=90.16%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.0001, act=sigmoid, loss=huber\n",
            "Val acc=92.34% | Test acc=90.42%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.001, act=relu, loss=cross_entropy\n",
            "Val acc=95.10% | Test acc=93.56%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.001, act=relu, loss=huber\n",
            "Val acc=95.04% | Test acc=93.81%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.001, act=tanh, loss=cross_entropy\n",
            "Val acc=94.00% | Test acc=92.70%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.001, act=tanh, loss=huber\n",
            "Val acc=94.06% | Test acc=92.63%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=91.78% | Test acc=89.76%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.0, wd=0.001, act=sigmoid, loss=huber\n",
            "Val acc=91.90% | Test acc=89.99%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.0, act=relu, loss=cross_entropy\n",
            "Val acc=97.68% | Test acc=96.93%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.0, act=relu, loss=huber\n",
            "Val acc=97.18% | Test acc=96.60%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.0, act=tanh, loss=cross_entropy\n",
            "Val acc=97.22% | Test acc=96.69%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.0, act=tanh, loss=huber\n",
            "Val acc=97.24% | Test acc=96.71%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.0, act=sigmoid, loss=cross_entropy\n",
            "Val acc=96.04% | Test acc=94.60%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.0, act=sigmoid, loss=huber\n",
            "Val acc=95.80% | Test acc=94.47%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.0001, act=relu, loss=cross_entropy\n",
            "Val acc=97.52% | Test acc=97.30%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.0001, act=relu, loss=huber\n",
            "Val acc=97.90% | Test acc=97.26%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.0001, act=tanh, loss=cross_entropy\n",
            "Val acc=97.38% | Test acc=96.82%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.0001, act=tanh, loss=huber\n",
            "Val acc=97.10% | Test acc=96.77%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.0001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=95.64% | Test acc=93.91%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.0001, act=sigmoid, loss=huber\n",
            "Val acc=95.58% | Test acc=94.17%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.001, act=relu, loss=cross_entropy\n",
            "Val acc=96.96% | Test acc=95.94%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.001, act=relu, loss=huber\n",
            "Val acc=96.88% | Test acc=96.20%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.001, act=tanh, loss=cross_entropy\n",
            "Val acc=96.20% | Test acc=94.68%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.001, act=tanh, loss=huber\n",
            "Val acc=96.20% | Test acc=94.70%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=92.86% | Test acc=91.20%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.9, wd=0.001, act=sigmoid, loss=huber\n",
            "Val acc=92.82% | Test acc=91.24%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.0, act=relu, loss=cross_entropy\n",
            "Val acc=80.96% | Test acc=79.03%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.0, act=relu, loss=huber\n",
            "Val acc=77.08% | Test acc=73.49%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.0, act=tanh, loss=cross_entropy\n",
            "Val acc=93.22% | Test acc=91.54%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.0, act=tanh, loss=huber\n",
            "Val acc=93.22% | Test acc=91.22%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.0, act=sigmoid, loss=cross_entropy\n",
            "Val acc=97.40% | Test acc=96.98%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.0, act=sigmoid, loss=huber\n",
            "Val acc=97.50% | Test acc=96.98%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.0001, act=relu, loss=cross_entropy\n",
            "Val acc=81.60% | Test acc=79.93%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.0001, act=relu, loss=huber\n",
            "Val acc=87.70% | Test acc=85.61%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.0001, act=tanh, loss=cross_entropy\n",
            "Val acc=92.42% | Test acc=90.13%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.0001, act=tanh, loss=huber\n",
            "Val acc=92.34% | Test acc=90.58%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.0001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=96.84% | Test acc=96.10%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.0001, act=sigmoid, loss=huber\n",
            "Val acc=96.72% | Test acc=95.89%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.001, act=relu, loss=cross_entropy\n",
            "Val acc=87.14% | Test acc=83.98%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.001, act=relu, loss=huber\n",
            "Val acc=86.68% | Test acc=83.26%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.001, act=tanh, loss=cross_entropy\n",
            "Val acc=88.80% | Test acc=87.32%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.001, act=tanh, loss=huber\n",
            "Val acc=89.72% | Test acc=88.51%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.001, act=sigmoid, loss=cross_entropy\n",
            "Val acc=90.92% | Test acc=89.27%\n",
            "\n",
            ">>> Training with lr=0.1, mom=0.99, wd=0.001, act=sigmoid, loss=huber\n",
            "Val acc=90.80% | Test acc=88.54%\n",
            "\n",
            "===== Summary of Experiments =====\n",
            "       lr  momentum   decay activation           loss  val_acc  test_acc\n",
            "90   0.01      0.99  0.0000       relu  cross_entropy    97.94     97.22\n",
            "133  0.10      0.90  0.0001       relu          huber    97.90     97.26\n",
            "93   0.01      0.99  0.0000       tanh          huber    97.68     96.70\n",
            "97   0.01      0.99  0.0001       relu          huber    97.68     97.16\n",
            "126  0.10      0.90  0.0000       relu  cross_entropy    97.68     96.93\n",
            "96   0.01      0.99  0.0001       relu  cross_entropy    97.60     96.85\n",
            "132  0.10      0.90  0.0001       relu  cross_entropy    97.52     97.30\n",
            "92   0.01      0.99  0.0000       tanh  cross_entropy    97.50     96.80\n",
            "149  0.10      0.99  0.0000    sigmoid          huber    97.50     96.98\n",
            "148  0.10      0.99  0.0000    sigmoid  cross_entropy    97.40     96.98\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# Imports & Utilities\n",
        "# =========================================================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from typing import List, Dict\n",
        "\n",
        "try:\n",
        "    from tensorflow.keras.datasets import mnist, cifar10\n",
        "    tf_available = True\n",
        "except:\n",
        "    tf_available = False\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---------- Utils ----------\n",
        "def one_hot(labels: np.ndarray, num_classes: int) -> np.ndarray:\n",
        "    y = np.zeros((labels.shape[0], num_classes), dtype=np.float32)\n",
        "    for i, lab in enumerate(labels):\n",
        "        y[i, int(lab)] = 1.0\n",
        "    return y\n",
        "\n",
        "def stable_softmax(x: np.ndarray) -> np.ndarray:\n",
        "    x_max = np.max(x, axis=1, keepdims=True)\n",
        "    ex = np.exp(x - x_max)\n",
        "    return ex / np.sum(ex, axis=1, keepdims=True)\n",
        "\n",
        "def accuracy(pred_probs: np.ndarray, labels: np.ndarray) -> float:\n",
        "    preds = np.argmax(pred_probs, axis=1)\n",
        "    return np.mean(preds == labels) * 100.0\n",
        "\n",
        "# =========================================================\n",
        "# Activations\n",
        "# =========================================================\n",
        "def linear(x): return x\n",
        "def d_linear(out): return np.ones_like(out)\n",
        "\n",
        "def sigmoid(x): return 1.0 / (1.0 + np.exp(-x))\n",
        "def d_sigmoid(out): return out * (1.0 - out)\n",
        "\n",
        "def tanh_act(x): return np.tanh(x)\n",
        "def d_tanh(out): return 1.0 - out**2\n",
        "\n",
        "def relu(x): return np.maximum(0, x)\n",
        "def d_relu(out): return (out > 0).astype(np.float32)\n",
        "\n",
        "def leaky_relu(x, alpha=0.01): return np.where(x >= 0, x, alpha * x)\n",
        "def d_leaky_relu_out(out, alpha=0.01):\n",
        "    deriv = np.ones_like(out)\n",
        "    deriv[out < 0] = alpha\n",
        "    return deriv\n",
        "\n",
        "def softmax_jacobian(s: np.ndarray) -> np.ndarray:\n",
        "    s = s.reshape(-1,1)\n",
        "    return np.diagflat(s) - s.dot(s.T)\n",
        "\n",
        "def d_softmax(prev_grad: np.ndarray, softmax_out: np.ndarray) -> np.ndarray:\n",
        "    batch = prev_grad.shape[0]\n",
        "    out = np.zeros_like(prev_grad)\n",
        "    for i in range(batch):\n",
        "        J = softmax_jacobian(softmax_out[i])\n",
        "        out[i] = prev_grad[i].dot(J)\n",
        "    return out\n",
        "\n",
        "ACT_FNS = {\n",
        "    'linear': (linear, d_linear),\n",
        "    'sigmoid': (sigmoid, d_sigmoid),\n",
        "    'tanh': (tanh_act, d_tanh),\n",
        "    'relu': (relu, d_relu),\n",
        "    'leaky_relu': (leaky_relu, d_leaky_relu_out),\n",
        "}\n",
        "\n",
        "# =========================================================\n",
        "# Losses\n",
        "# =========================================================\n",
        "def cross_entropy_loss(probs: np.ndarray, targets: np.ndarray) -> float:\n",
        "    eps = 1e-12\n",
        "    probs_clipped = np.clip(probs, eps, 1. - eps)\n",
        "    return -np.sum(targets * np.log(probs_clipped)) / probs.shape[0]\n",
        "\n",
        "def d_cross_entropy(probs: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
        "    batch = probs.shape[0]\n",
        "    return (probs - targets) / batch\n",
        "\n",
        "def huber_loss(pred: np.ndarray, target: np.ndarray, delta=1.0):\n",
        "    diff = pred - target\n",
        "    absd = np.abs(diff)\n",
        "    mask = absd <= delta\n",
        "    loss = np.where(mask, 0.5 * diff**2, delta * (absd - 0.5 * delta))\n",
        "    return np.mean(loss)\n",
        "\n",
        "def d_huber(pred: np.ndarray, target: np.ndarray, delta=1.0):\n",
        "    diff = pred - target\n",
        "    absd = np.abs(diff)\n",
        "    mask = absd <= delta\n",
        "    grad = np.where(mask, diff, delta * np.sign(diff))\n",
        "    batch = pred.shape[0]\n",
        "    return grad / batch\n",
        "\n",
        "# =========================================================\n",
        "# Layers & Model\n",
        "# =========================================================\n",
        "class DenseLayer:\n",
        "    def __init__(self, input_dim:int, output_dim:int, activation='relu', init='xavier'):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.activation_name = activation\n",
        "        self.activation, self.d_activation = ACT_FNS.get(activation, ACT_FNS['relu'])\n",
        "\n",
        "        # init weights\n",
        "        if init == 'xavier':\n",
        "            limit = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "            self.W = np.random.uniform(-limit, limit, size=(input_dim, output_dim)).astype(np.float32)\n",
        "        elif init == 'he':\n",
        "            std = np.sqrt(2.0 / input_dim)\n",
        "            self.W = np.random.randn(input_dim, output_dim).astype(np.float32) * std\n",
        "        else:\n",
        "            self.W = np.random.randn(input_dim, output_dim).astype(np.float32) * 0.01\n",
        "\n",
        "        self.b = np.zeros((1, output_dim), dtype=np.float32)\n",
        "\n",
        "        # gradients & velocity\n",
        "        self.dW = np.zeros_like(self.W)\n",
        "        self.db = np.zeros_like(self.b)\n",
        "        self.vW = np.zeros_like(self.W)\n",
        "        self.vb = np.zeros_like(self.b)\n",
        "\n",
        "        self.x = None\n",
        "        self.z = None\n",
        "        self.a = None\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.x = x\n",
        "        self.z = x.dot(self.W) + self.b\n",
        "        if self.activation_name == 'softmax':\n",
        "            self.a = stable_softmax(self.z)\n",
        "        else:\n",
        "            fn = ACT_FNS[self.activation_name][0]\n",
        "            self.a = fn(self.z)\n",
        "        return self.a\n",
        "\n",
        "    def backward(self, grad_a: np.ndarray) -> np.ndarray:\n",
        "        if self.activation_name == 'softmax':\n",
        "            grad_z = d_softmax(grad_a, self.a)\n",
        "        else:\n",
        "            dfn = ACT_FNS[self.activation_name][1]\n",
        "            grad_z = grad_a * dfn(self.a)\n",
        "\n",
        "        self.dW = self.x.T.dot(grad_z)\n",
        "        self.db = np.sum(grad_z, axis=0, keepdims=True)\n",
        "        grad_x = grad_z.dot(self.W.T)\n",
        "        return grad_x\n",
        "\n",
        "class SimpleModel:\n",
        "    def __init__(self, layers: List[DenseLayer]):\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        out = x\n",
        "        for layer in self.layers:\n",
        "            out = layer.forward(out)\n",
        "        return out\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> None:\n",
        "        grad = grad_output\n",
        "        for layer in reversed(self.layers):\n",
        "            grad = layer.backward(grad)\n",
        "\n",
        "    def correct_update(self, lr: float, momentum: float=0.9, decay: float=0.0):\n",
        "        for layer in self.layers:\n",
        "            layer.vW = momentum * layer.vW - lr * (layer.dW + decay * layer.W)\n",
        "            layer.vb = momentum * layer.vb - lr * (layer.db + decay * layer.b)\n",
        "            layer.W += layer.vW\n",
        "            layer.b += layer.vb\n",
        "\n",
        "# =========================================================\n",
        "# Training Loop\n",
        "# =========================================================\n",
        "def train_model(model: SimpleModel,\n",
        "                X_train: np.ndarray, y_train: np.ndarray,\n",
        "                X_val: np.ndarray, y_val: np.ndarray,\n",
        "                epochs: int = 10, batch_size: int = 64,\n",
        "                lr: float = 0.01, momentum: float = 0.9, decay: float = 0.0,\n",
        "                loss_name: str = 'cross_entropy', verbose: bool = True):\n",
        "\n",
        "    n = X_train.shape[0]\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        perm = np.random.permutation(n)\n",
        "        X_sh = X_train[perm]\n",
        "        y_sh = y_train[perm]\n",
        "        epoch_loss, epoch_acc = 0.0, 0.0\n",
        "        t0 = time()\n",
        "\n",
        "        for i in range(0, n, batch_size):\n",
        "            xb = X_sh[i:i+batch_size]\n",
        "            yb = y_sh[i:i+batch_size]\n",
        "            preds = model.forward(xb)\n",
        "\n",
        "            if loss_name == 'cross_entropy':\n",
        "                loss = cross_entropy_loss(preds, yb)\n",
        "                grad = d_cross_entropy(preds, yb)\n",
        "            elif loss_name == 'huber':\n",
        "                loss = huber_loss(preds, yb)\n",
        "                grad = d_huber(preds, yb)\n",
        "            else:\n",
        "                raise ValueError('Unknown loss')\n",
        "\n",
        "            epoch_loss += loss * xb.shape[0]\n",
        "            epoch_acc += accuracy(preds, np.argmax(yb, axis=1)) * xb.shape[0]\n",
        "\n",
        "            model.backward(grad)\n",
        "            model.correct_update(lr, momentum, decay)\n",
        "\n",
        "        epoch_loss /= n\n",
        "        epoch_acc /= n\n",
        "        val_preds = model.forward(X_val)\n",
        "        val_loss = cross_entropy_loss(val_preds, y_val) if loss_name=='cross_entropy' else huber_loss(val_preds,y_val)\n",
        "        val_acc = accuracy(val_preds, np.argmax(y_val, axis=1))\n",
        "\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_acc'].append(epoch_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Epoch {ep}/{epochs} - loss: {epoch_loss:.4f} - acc: {epoch_acc:.2f}% - val_loss: {val_loss:.4f} - val_acc: {val_acc:.2f}% - time: {time()-t0:.1f}s')\n",
        "\n",
        "    return history\n",
        "\n",
        "# =========================================================\n",
        "# Load MNIST\n",
        "# =========================================================\n",
        "if tf_available:\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    X_train = X_train.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
        "    X_test  = X_test.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
        "    X_val = X_train[-5000:]; y_val = y_train[-5000:]\n",
        "    X_train = X_train[:-5000]; y_train = y_train[:-5000]\n",
        "    y_train_oh = one_hot(y_train, 10)\n",
        "    y_val_oh   = one_hot(y_val, 10)\n",
        "    y_test_oh  = one_hot(y_test, 10)\n",
        "    print('MNIST loaded:', X_train.shape, X_val.shape, X_test.shape)\n",
        "else:\n",
        "    print(\"TensorFlow datasets not available.\")\n",
        "\n",
        "# =========================================================\n",
        "# Hyperparameter Experiments\n",
        "# =========================================================\n",
        "import pandas as pd\n",
        "results = []\n",
        "\n",
        "lrs = [1e-3, 1e-2, 1e-1]\n",
        "momentums = [0.0, 0.9, 0.99]\n",
        "decays = [0.0, 1e-4, 1e-3]\n",
        "acts = ['relu', 'tanh', 'sigmoid']\n",
        "losses = ['cross_entropy', 'huber']\n",
        "\n",
        "for lr in lrs:\n",
        "    for mom in momentums:\n",
        "        for wd in decays:\n",
        "            for act in acts:\n",
        "                for loss in losses:\n",
        "                    print(f\"\\n>>> Training with lr={lr}, mom={mom}, wd={wd}, act={act}, loss={loss}\")\n",
        "                    model = SimpleModel([\n",
        "                        DenseLayer(784, 128, activation=act, init='he'),\n",
        "                        DenseLayer(128, 10, activation='softmax', init='xavier')\n",
        "                    ])\n",
        "                    history = train_model(model,\n",
        "                                          X_train, y_train_oh,\n",
        "                                          X_val, y_val_oh,\n",
        "                                          epochs=5,  # adjust to 10+ for full runs\n",
        "                                          lr=lr, momentum=mom, decay=wd,\n",
        "                                          loss_name=loss, verbose=False)\n",
        "                    val_acc = history['val_acc'][-1]\n",
        "                    test_acc = accuracy(model.forward(X_test), y_test)\n",
        "                    results.append({\n",
        "                        'lr': lr, 'momentum': mom, 'decay': wd,\n",
        "                        'activation': act, 'loss': loss,\n",
        "                        'val_acc': val_acc, 'test_acc': test_acc\n",
        "                    })\n",
        "                    print(f\"Val acc={val_acc:.2f}% | Test acc={test_acc:.2f}%\")\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(\"\\n===== Summary of Experiments =====\")\n",
        "print(df.sort_values(by='val_acc', ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d06f66f",
      "metadata": {
        "id": "8d06f66f"
      },
      "source": [
        "## 9) Submission\n",
        "\n",
        "Turn in the Colab notebook (`.ipynb`) with the following:\n",
        "- Completed implementations (no TODOs left)\n",
        "- Training plots and short answers to observations (learning curves, stability issues)\n",
        "- `activations.py`, `model.py` (if you split code into files) and `questions.txt` explaining your experiments and results.\n",
        "\n",
        "---\n",
        "\n",
        "**Instructor hint:** you may add unit tests that run small shapes through activations and gradients to verify correctness automatically."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}